{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277139d2-03d9-4b55-aa3f-9596d3ce4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_list=list(train_set.iloc[:,0])\n",
    "test_set_list=list(test_set.iloc[:,0])\n",
    "dev_set_list=list(dev_set.iloc[:,0])\n",
    "\n",
    "train_labels=np.array(train_set.iloc[:,1])\n",
    "test_labels=np.array(test_set.iloc[:,1])\n",
    "dev_labels=np.array(dev_set.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec12c0c-e780-4d5e-9ce1-640e6a2b289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (6.2.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (3.0.1)\n",
      "Requirement already satisfied: traitlets>=5.0 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (5.1.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (2.10.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (0.5.4)\n",
      "Requirement already satisfied: jupyter-core in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (4.8.1)\n",
      "Requirement already satisfied: testpath in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (0.5.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (0.8.4)\n",
      "Requirement already satisfied: nbformat>=4.4 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (5.1.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: bleach in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (4.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbconvert) (1.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from jinja2>=2.4->nbconvert) (2.0.1)\n",
      "Requirement already satisfied: nest-asyncio in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (7.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: tornado>=4.1 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (6.1)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (22.3.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from jupyter-core->nbconvert) (301)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbformat>=4.4->nbconvert) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from nbformat>=4.4->nbconvert) (0.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (58.0.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (0.17.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from bleach->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from bleach->nbconvert) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\jupyterlab\\resources\\jlab_server\\lib\\site-packages (from packaging->bleach->nbconvert) (2.4.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd41157-760b-40cc-99b9-3eb857695cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_list=[]\n",
    "test_set_list=[]\n",
    "dev_set_list=[]\n",
    "\n",
    "for i in range(train_set.shape[0]):\n",
    "    line_text = train_set.iloc[i,0]\n",
    "    line_stripped = line_text.strip()\n",
    "    line_list = line_stripped.split()\n",
    "    train_set_list.append(line_list)\n",
    "for i in range(test_set.shape[0]):\n",
    "    line_text = test_set.iloc[i,0]\n",
    "    line_stripped = line_text.strip()\n",
    "    line_list = line_stripped.split()\n",
    "    test_set_list.append(line_list)\n",
    "for i in range(dev_set.shape[0]):\n",
    "    line_text = dev_set.iloc[i,0]\n",
    "    line_stripped = line_text.strip()\n",
    "    line_list = line_stripped.split()\n",
    "    dev_set_list.append(line_list)\n",
    "    \n",
    "train_labels=np.array(train_set.iloc[:,1])\n",
    "test_labels=np.array(test_set.iloc[:,1])\n",
    "dev_labels=np.array(dev_set.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1171f-e450-4909-8df3-273ddb274489",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text=re.findall('[A-Za-z0-9\\']+(?:\\`[A-Za-z]+)?',train_set_list[0])\n",
    "filtered_sentence = [word for word in split_text if not word in stopWords]\n",
    "filtered_sentence\n",
    "vo=[]\n",
    "for w in filtered_sentence: vo.append(w)\n",
    "datf=dict()\n",
    "for w in filtered_sentence:\n",
    "    if w not in datf:\n",
    "        datf[w]=1\n",
    "    else:\n",
    "        datf[w]=datf[w]+1\n",
    "#datf\n",
    "vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ba838-8b3a-494c-b200-1b72f3b7d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in temp_set:\n",
    "            if word not in df:\n",
    "                df[word]=1\n",
    "            else:\n",
    "                df[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705971e-b998-4e89-ad7a-3d2b1d0ce763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'', \n",
    "              min_df=0, keep_topN=0, \n",
    "              stop_words1=[],char_ngrams=False):\n",
    "    vocab=set()\n",
    "    df=dict()\n",
    "    ngram_counts=dict()\n",
    "    for i in range(len(X_raw)):\n",
    "        split_sentence = extract_ngrams(X_raw[i], stop_words2=stop_words1)\n",
    "        temp_list=list()\n",
    "        for word in split_sentence: \n",
    "            temp_list.append(word)\n",
    "            #if word in ngram_counts.keys(): ngram_counts[word]+=1\n",
    "            #else: ngram_counts[word]=1\n",
    "        for word in set(temp_list):\n",
    "            if word in df: df[word]+=1\n",
    "            else: df[word]=1\n",
    "    for i in list(df.keys()):\n",
    "        if df[i]<min_df: del df[i]\n",
    "    vocab=sorted(list(df.keys()))\n",
    "    for j in range(len(vocab)):\n",
    "        word=vocab[j]\n",
    "        ngram_counts[word]=0\n",
    "        for k in range(len(X_raw)):\n",
    "            doc=X_raw[k]\n",
    "            if word in doc: ngram_counts[word]+=doc.count(word)\n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c403114-8759-4c61-a736-e1e111448349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "phrase = \"Mary had a little lamb\"\n",
    "print(pd.Series(list(phrase)).value_counts()['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb66a26-dcfb-4c5a-9061-45fec8d8871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'', \n",
    "                   stop_words2=[], vocab=set(), char_ngrams=False):\n",
    "    x_raw=x_raw.lower()\n",
    "    split_text=re.findall('[A-Za-z0-9\\']+(?:\\`[A-Za-z]+)?',x_raw)\n",
    "    x, bigrams, trigrams=[],[],[]\n",
    "    unigrams = [word for word in split_text if not word in stop_words2]\n",
    "    \n",
    "    if char_ngrams==True:\n",
    "        char_2ngrams, char_3ngrams= [],[]\n",
    "        for word in unigrams:\n",
    "            letters=list(word)\n",
    "            n2grams= (list(zip(*[letters[i:] for i in range(2)])))\n",
    "            for w in n2grams: char_2ngrams.append(''.join(w))\n",
    "            #if (vocab)!=set(): n2grams= [word for word in char_2ngrams if word in vocab]\n",
    "            x.append(char_2ngrams)\n",
    "            #n3grams= (list(zip(*[letters[i:] for i in range(3)])))\n",
    "            #for w in n3grams: char_3ngrams.append(''.join(w))\n",
    "            #x.append(char_3ngrams)\n",
    "        x = [item for sub_list in x for item in sub_list]    # unlist items and make a single list  \n",
    "        #if vocab!=set(): x = [word for word in x if word in vocab]\n",
    "    else:        \n",
    "        x.append(unigrams)\n",
    "        bi_grams = list(zip(*[unigrams[i:] for i in range(2)]))\n",
    "        for w in (bi_grams): bigrams.append(' '.join(w))\n",
    "        x.append(bigrams)\n",
    "        tri_grams = list(zip(*[unigrams[i:] for i in range(3)]))\n",
    "        for w in (tri_grams): trigrams.append(' '.join(w))\n",
    "        x.append(trigrams)\n",
    "        x = [item for sub_list in x for item in sub_list]    # unlist items and make a single list\n",
    "        if vocab!=set(): x = [word for word in x if word in vocab]\n",
    "    return x\n",
    "for j in range(1395): #(len(train_set_list)):\n",
    "    a=extract_ngrams(train_set_list[j], stop_words2=stop_words,char_ngrams=True, vocab=vocab_char)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c1f5a78-ca49-49f7-8240-83db69e824ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=['note',\n",
    " 'may',\n",
    " 'consider',\n",
    " 'portions',\n",
    " 'following',\n",
    " 'text',\n",
    " 'spoilers',\n",
    " 'forewarned',\n",
    " 'among',\n",
    " 'fanatical',\n",
    " 'ticker',\n",
    " 'tape',\n",
    " 'worshipping',\n",
    " 'friends',\n",
    " \"there's\",\n",
    " 'one',\n",
    " 'happens',\n",
    " 'share',\n",
    " 'philosophy',\n",
    " 'espoused',\n",
    " 'central',\n",
    " 'character',\n",
    " 'darren',\n",
    " \"aronofsky's\",\n",
    " 'darkly',\n",
    " 'original',\n",
    " 'pi',\n",
    " 'entire',\n",
    " 'stock',\n",
    " 'market',\n",
    " 'reduced',\n",
    " 'nothing',\n",
    " 'but',\n",
    " 'series',\n",
    " 'patterns',\n",
    " 'analysis',\n",
    " 'produce',\n",
    " 'information',\n",
    " 'accurately',\n",
    " 'forecast',\n",
    " 'future',\n",
    " 'behaviour',\n",
    " 'example',\n",
    " 'mentality',\n",
    " 'involved',\n",
    " 'stock',\n",
    " 'price',\n",
    " 'goes',\n",
    " 'like',\n",
    " 'down',\n",
    " 'like',\n",
    " 'sharply',\n",
    " 'way',\n",
    " 'go',\n",
    " 'way',\n",
    " 'freely',\n",
    " 'admit',\n",
    " 'know',\n",
    " 'less',\n",
    " 'nothing',\n",
    " 'market',\n",
    " 'knowledge',\n",
    " 'check',\n",
    " 'prices',\n",
    " 'good',\n",
    " 'prices',\n",
    " 'down',\n",
    " 'bad',\n",
    " 'time',\n",
    " 'least',\n",
    " 'hence',\n",
    " 'really',\n",
    " \"couldn't\",\n",
    " 'comment',\n",
    " 'authority',\n",
    " \"it's\",\n",
    " 'always',\n",
    " 'nonetheless',\n",
    " 'struck',\n",
    " 'incredibly',\n",
    " 'naive',\n",
    " 'oversimplification',\n",
    " 'astonishingly',\n",
    " 'complex',\n",
    " 'system',\n",
    " 'besides',\n",
    " 'simple',\n",
    " 'no',\n",
    " 'doubt',\n",
    " 'somebody',\n",
    " \"would've\",\n",
    " 'already',\n",
    " 'figured',\n",
    " 'difference',\n",
    " 'case',\n",
    " 'colleague',\n",
    " 'otherwise',\n",
    " 'assuredly',\n",
    " 'realistic',\n",
    " 'individual',\n",
    " 'truly',\n",
    " 'believes',\n",
    " 'valid',\n",
    " 'forecaster',\n",
    " 'pi',\n",
    " 'uses',\n",
    " 'ideology',\n",
    " 'device',\n",
    " 'investigate',\n",
    " \"character's\",\n",
    " 'psychosis',\n",
    " \"it's\",\n",
    " 'also',\n",
    " 'vastly',\n",
    " 'convincing',\n",
    " 'argument',\n",
    " 'mathematics',\n",
    " 'language',\n",
    " 'universe',\n",
    " 'insists',\n",
    " 'genius',\n",
    " 'protagonist',\n",
    " 'maximillian',\n",
    " 'cohen',\n",
    " 'sean',\n",
    " 'gullette',\n",
    " 'cool',\n",
    " 'mantra',\n",
    " 'like',\n",
    " 'voice',\n",
    " 'repeats',\n",
    " 'throughout',\n",
    " 'picture',\n",
    " 'since',\n",
    " 'nature',\n",
    " 'expressed',\n",
    " 'numbers',\n",
    " 'patterns',\n",
    " 'everywhere',\n",
    " 'nature',\n",
    " 'reasons',\n",
    " 'eminent',\n",
    " 'logic',\n",
    " 'finding',\n",
    " 'patterns',\n",
    " 'allow',\n",
    " 'predict',\n",
    " 'anything',\n",
    " 'ups',\n",
    " 'downs',\n",
    " 'stock',\n",
    " 'market',\n",
    " 'many',\n",
    " 'games',\n",
    " 'yankees',\n",
    " 'win',\n",
    " 'year',\n",
    " 'flavour',\n",
    " 'jam',\n",
    " \"i'm\",\n",
    " 'going',\n",
    " 'put',\n",
    " 'toast',\n",
    " 'tomorrow',\n",
    " 'morning',\n",
    " 'obsessed',\n",
    " 'finding',\n",
    " 'proverbial',\n",
    " 'key',\n",
    " 'universe',\n",
    " 'max',\n",
    " 'lives',\n",
    " 'paranoid',\n",
    " 'self',\n",
    " 'imposed',\n",
    " 'solitude',\n",
    " 'seedy',\n",
    " 'nyc',\n",
    " 'chinatown',\n",
    " 'apartment',\n",
    " 'single',\n",
    " 'mindedly',\n",
    " 'toiling',\n",
    " 'away',\n",
    " 'monstrous',\n",
    " 'homemade',\n",
    " 'computer',\n",
    " 'system',\n",
    " 'sullenly',\n",
    " 'withdrawn',\n",
    " 'plauged',\n",
    " 'debilitating',\n",
    " 'migraines',\n",
    " 'elusive',\n",
    " 'pursuit',\n",
    " 'mysterious',\n",
    " '216',\n",
    " 'digit',\n",
    " 'number',\n",
    " 'machine',\n",
    " 'spits',\n",
    " 'one',\n",
    " 'day',\n",
    " 'driving',\n",
    " 'madness',\n",
    " 'story',\n",
    " 'basically',\n",
    " 'eccentricity',\n",
    " 'but',\n",
    " \"it's\",\n",
    " 'clever',\n",
    " 'astute',\n",
    " 'eccentricity',\n",
    " 'perceptively',\n",
    " 'zeroing',\n",
    " 'modern',\n",
    " 'mistrust',\n",
    " 'mathematical',\n",
    " 'reductionism',\n",
    " 'age',\n",
    " 'dominant',\n",
    " 'societal',\n",
    " 'phobia',\n",
    " \"one's\",\n",
    " 'individualism',\n",
    " 'replaced',\n",
    " 'series',\n",
    " 'numeric',\n",
    " 'identifiers',\n",
    " \"max's\",\n",
    " 'consuming',\n",
    " 'penchent',\n",
    " 'numbers',\n",
    " 'creates',\n",
    " 'lingering',\n",
    " 'unsettling',\n",
    " 'mood',\n",
    " 'helps',\n",
    " 'matters',\n",
    " \"he's\",\n",
    " 'not',\n",
    " 'particularly',\n",
    " 'likable',\n",
    " 'protagonist',\n",
    " 'attempts',\n",
    " 'friendliness',\n",
    " 'neighbours',\n",
    " 'curtly',\n",
    " 'rebuffed',\n",
    " 'max',\n",
    " 'spindly',\n",
    " 'neurotic',\n",
    " 'looking',\n",
    " 'individual',\n",
    " \"hasn't\",\n",
    " 'time',\n",
    " 'indulge',\n",
    " 'pleasantries',\n",
    " 'film',\n",
    " 'puts',\n",
    " 'lead',\n",
    " 'character',\n",
    " 'front',\n",
    " 'center',\n",
    " 'mr',\n",
    " 'gullette',\n",
    " 'appears',\n",
    " 'virtually',\n",
    " 'every',\n",
    " 'scene',\n",
    " 'pi',\n",
    " 'takes',\n",
    " 'refreshing',\n",
    " 'effective',\n",
    " 'approach',\n",
    " 'avoiding',\n",
    " 'conventional',\n",
    " 'aesthetics',\n",
    " 'ambivalence',\n",
    " 'max',\n",
    " \"we're\",\n",
    " 'not',\n",
    " 'much',\n",
    " 'avidly',\n",
    " 'rooting',\n",
    " 'triumph',\n",
    " 'moment',\n",
    " 'epiphany',\n",
    " \"we're\",\n",
    " 'following',\n",
    " 'plot',\n",
    " 'sense',\n",
    " 'mixed',\n",
    " 'dread',\n",
    " 'morbid',\n",
    " 'fascination',\n",
    " \"it's\",\n",
    " 'disturbing',\n",
    " 'journey',\n",
    " 'quest',\n",
    " 'still',\n",
    " 'care',\n",
    " \"max's\",\n",
    " 'fate',\n",
    " 'teetering',\n",
    " 'edge',\n",
    " 'dementia',\n",
    " 'winds',\n",
    " 'pursued',\n",
    " 'two',\n",
    " 'different',\n",
    " 'groups',\n",
    " 'want',\n",
    " 'pick',\n",
    " 'brain',\n",
    " 'fronted',\n",
    " 'deliciously',\n",
    " 'perky',\n",
    " 'resolutely',\n",
    " 'cheerful',\n",
    " 'representatives',\n",
    " 'inevitably',\n",
    " 'duplictious',\n",
    " 'intentions',\n",
    " 'know',\n",
    " 'films',\n",
    " 'paranoia',\n",
    " 'dominant',\n",
    " 'element',\n",
    " 'see',\n",
    " 'truman',\n",
    " \"show's\",\n",
    " 'laura',\n",
    " 'linney',\n",
    " 'character',\n",
    " 'matter',\n",
    " 'real',\n",
    " 'life',\n",
    " \"it's\",\n",
    " 'always',\n",
    " 'ones',\n",
    " 'never',\n",
    " 'stop',\n",
    " 'smiling',\n",
    " 'overly',\n",
    " 'friendly',\n",
    " 'ones',\n",
    " 'wary',\n",
    " 'pi',\n",
    " 'film',\n",
    " 'addresses',\n",
    " 'patterns',\n",
    " 'intentionally',\n",
    " 'adheres',\n",
    " 'identifiable',\n",
    " 'pattern',\n",
    " 'cycle',\n",
    " 'headache',\n",
    " 'scene',\n",
    " 'important',\n",
    " 'revelation',\n",
    " 'bit',\n",
    " 'plot',\n",
    " 'development',\n",
    " 'pill',\n",
    " 'popping',\n",
    " 'montage',\n",
    " 'hallucinatory',\n",
    " 'nightmare',\n",
    " 'decidedly',\n",
    " 'cronenberg',\n",
    " 'esque',\n",
    " 'undertones',\n",
    " 'directors',\n",
    " 'equally',\n",
    " 'adept',\n",
    " 'bridging',\n",
    " 'unsettling',\n",
    " 'concepts',\n",
    " 'body',\n",
    " 'themed',\n",
    " 'horror',\n",
    " 'nosebleeding',\n",
    " 'reality',\n",
    " 'repetitiveness',\n",
    " 'far',\n",
    " 'tedious',\n",
    " 'effectively',\n",
    " 'maddening',\n",
    " 'anything',\n",
    " 'picture',\n",
    " 'aims',\n",
    " 'get',\n",
    " 'skins',\n",
    " 'take',\n",
    " 'events',\n",
    " \"max's\",\n",
    " 'claustrophobic',\n",
    " 'perspective',\n",
    " 'regard',\n",
    " 'wildly',\n",
    " 'succeeds',\n",
    " 'due',\n",
    " 'mr',\n",
    " \"aronofsky's\",\n",
    " 'striking',\n",
    " 'direction',\n",
    " \"it's\",\n",
    " 'rarity',\n",
    " 'film',\n",
    " 'completely',\n",
    " 'immerses',\n",
    " \"protagonist's\",\n",
    " 'warped',\n",
    " 'perspective',\n",
    " 'surrounding',\n",
    " 'high',\n",
    " 'contrast',\n",
    " 'black',\n",
    " 'white',\n",
    " 'cinematography',\n",
    " 'combined',\n",
    " 'constant',\n",
    " 'usage',\n",
    " 'extreme',\n",
    " 'close',\n",
    " 'ups',\n",
    " 'lend',\n",
    " 'heightened',\n",
    " 'sense',\n",
    " 'paranoia',\n",
    " 'proceedings',\n",
    " 'scenes',\n",
    " 'stark',\n",
    " 'composition',\n",
    " 'conjunction',\n",
    " 'lumbering',\n",
    " 'approach',\n",
    " 'mr',\n",
    " 'gullette',\n",
    " 'make',\n",
    " 'character',\n",
    " 'curiously',\n",
    " 'resemble',\n",
    " 'latter',\n",
    " 'day',\n",
    " 'max',\n",
    " 'schreck',\n",
    " 'nosferatu',\n",
    " 'using',\n",
    " 'savage',\n",
    " 'jittery',\n",
    " 'lensing',\n",
    " 'rapid',\n",
    " 'cuts',\n",
    " 'create',\n",
    " 'sense',\n",
    " 'disorientation',\n",
    " 'picture',\n",
    " 'often',\n",
    " 'dizzying',\n",
    " 'behold',\n",
    " \"max's\",\n",
    " 'effective',\n",
    " 'isolationism',\n",
    " 'emphasized',\n",
    " 'shots',\n",
    " 'called',\n",
    " 'snorri',\n",
    " 'cam',\n",
    " 'keep',\n",
    " 'plain',\n",
    " 'focus',\n",
    " 'environment',\n",
    " 'races',\n",
    " 'blurred',\n",
    " 'bursts',\n",
    " \"pi's\",\n",
    " 'raw',\n",
    " 'aggressive',\n",
    " 'visuals',\n",
    " 'reminiscent',\n",
    " 'david',\n",
    " \"lynch's\",\n",
    " 'early',\n",
    " 'work',\n",
    " 'particular',\n",
    " 'eraserhead',\n",
    " \"film's\",\n",
    " 'sinister',\n",
    " 'tone',\n",
    " 'splashes',\n",
    " 'onto',\n",
    " 'screen',\n",
    " 'immediately',\n",
    " 'dazzling',\n",
    " 'opening',\n",
    " 'credit',\n",
    " 'sequence',\n",
    " 'ably',\n",
    " 'backed',\n",
    " 'sly',\n",
    " 'electronic',\n",
    " 'score',\n",
    " 'clint',\n",
    " 'mansell',\n",
    " 'gradually',\n",
    " 'increases',\n",
    " 'intensity',\n",
    " 'still',\n",
    " 'amidst',\n",
    " 'kafkaesque',\n",
    " 'qualities',\n",
    " 'overall',\n",
    " 'dispassionate',\n",
    " 'mood',\n",
    " 'pi',\n",
    " 'occasionally',\n",
    " 'display',\n",
    " 'sense',\n",
    " 'humour',\n",
    " 'one',\n",
    " 'point',\n",
    " 'marcy',\n",
    " 'dawson',\n",
    " 'pamela',\n",
    " 'hart',\n",
    " 'great',\n",
    " 'fun',\n",
    " 'entices',\n",
    " 'max',\n",
    " 'offer',\n",
    " 'invaluable',\n",
    " 'treasure',\n",
    " 'one',\n",
    " 'kind',\n",
    " 'computer',\n",
    " 'chip',\n",
    " \"isn't\",\n",
    " 'beautiful',\n",
    " 'coos',\n",
    " 'showcase',\n",
    " 'mr',\n",
    " \"aronofsky's\",\n",
    " 'technical',\n",
    " 'virtuosity',\n",
    " 'made',\n",
    " '60',\n",
    " '000',\n",
    " \"it's\",\n",
    " 'since',\n",
    " 'gone',\n",
    " 'capture',\n",
    " 'acclaim',\n",
    " '1998',\n",
    " 'sundance',\n",
    " 'film',\n",
    " 'festival',\n",
    " 'pi',\n",
    " 'intriguingly',\n",
    " 'cerebral',\n",
    " 'story',\n",
    " 'ironically',\n",
    " 'perhaps',\n",
    " 'purely',\n",
    " 'visceral',\n",
    " 'film',\n",
    " 'year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcd1ab7b-738b-4ab4-96b3-81977f6ec62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"note may consider portions following text spoilers forewarned among fanatical ticker tape worshipping friends there's one happens share philosophy espoused central character darren aronofsky's darkly original pi entire stock market reduced nothing but series patterns analysis produce information accurately forecast future behaviour example mentality involved stock price goes like down like sharply way go way freely admit know less nothing market knowledge check prices good prices down bad time least hence really couldn't comment authority it's always nonetheless struck incredibly naive oversimplification astonishingly complex system besides simple no doubt somebody would've already figured difference case colleague otherwise assuredly realistic individual truly believes valid forecaster pi uses ideology device investigate character's psychosis it's also vastly convincing argument mathematics language universe insists genius protagonist maximillian cohen sean gullette cool mantra like voice repeats throughout picture since nature expressed numbers patterns everywhere nature reasons eminent logic finding patterns allow predict anything ups downs stock market many games yankees win year flavour jam i'm going put toast tomorrow morning obsessed finding proverbial key universe max lives paranoid self imposed solitude seedy nyc chinatown apartment single mindedly toiling away monstrous homemade computer system sullenly withdrawn plauged debilitating migraines elusive pursuit mysterious 216 digit number machine spits one day driving madness story basically eccentricity but it's clever astute eccentricity perceptively zeroing modern mistrust mathematical reductionism age dominant societal phobia one's individualism replaced series numeric identifiers max's consuming penchent numbers creates lingering unsettling mood helps matters he's not particularly likable protagonist attempts friendliness neighbours curtly rebuffed max spindly neurotic looking individual hasn't time indulge pleasantries film puts lead character front center mr gullette appears virtually every scene pi takes refreshing effective approach avoiding conventional aesthetics ambivalence max we're not much avidly rooting triumph moment epiphany we're following plot sense mixed dread morbid fascination it's disturbing journey quest still care max's fate teetering edge dementia winds pursued two different groups want pick brain fronted deliciously perky resolutely cheerful representatives inevitably duplictious intentions know films paranoia dominant element see truman show's laura linney character matter real life it's always ones never stop smiling overly friendly ones wary pi film addresses patterns intentionally adheres identifiable pattern cycle headache scene important revelation bit plot development pill popping montage hallucinatory nightmare decidedly cronenberg esque undertones directors equally adept bridging unsettling concepts body themed horror nosebleeding reality repetitiveness far tedious effectively maddening anything picture aims get skins take events max's claustrophobic perspective regard wildly succeeds due mr aronofsky's striking direction it's rarity film completely immerses protagonist's warped perspective surrounding high contrast black white cinematography combined constant usage extreme close ups lend heightened sense paranoia proceedings scenes stark composition conjunction lumbering approach mr gullette make character curiously resemble latter day max schreck nosferatu using savage jittery lensing rapid cuts create sense disorientation picture often dizzying behold max's effective isolationism emphasized shots called snorri cam keep plain focus environment races blurred bursts pi's raw aggressive visuals reminiscent david lynch's early work particular eraserhead film's sinister tone splashes onto screen immediately dazzling opening credit sequence ably backed sly electronic score clint mansell gradually increases intensity still amidst kafkaesque qualities overall dispassionate mood pi occasionally display sense humour one point marcy dawson pamela hart great fun entices max offer invaluable treasure one kind computer chip isn't beautiful coos showcase mr aronofsky's technical virtuosity made 60 000 it's since gone capture acclaim 1998 sundance film festival pi intriguingly cerebral story ironically perhaps purely visceral film year\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06c7b932-3160-4e38-9e44-e78b89c9f3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J', 'a', 'g', 'a', 't', ' ', 'i', 's', ' ', 'a']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ja', 'ag', 'ga', 'at', 't ', ' i', 'is', 's ', ' a']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zipngram(text,n=2):\n",
    "    text=list(text)\n",
    "    print(text)\n",
    "    return [\"\".join(j) for j in zip(*[text[i:] for i in range(n)])]\n",
    "a=zipngram('Jagat is a')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9032128-9ea3-45eb-8af2-818fc4223122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4397"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rnd='Jagat'\n",
    "ttt=['note', 'may', 'consider', 'portions', 'following']\n",
    "b,c=[],[]\n",
    "e=[]\n",
    "for rnd in ' '.join(abc):\n",
    "    #c=[]\n",
    "    #a=(list(zip(*[rnd[i:] for i in range(2)])))\n",
    "    #for w in a: c.append(''.join(w))\n",
    "    #e.append(c)\n",
    "    d=[rnd[i:i+2] for i in range(len(rnd)-1)]\n",
    "    b.append(d)\n",
    "    #print(c==d)\n",
    "e==b\n",
    "len(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38fa3ba2-c7a5-4ee9-a156-685909f2a71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b==c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9be6fc53-ef80-42ea-87c0-7b1ddac1e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = [item for sub_list in e for item in sub_list]\n",
    "f = [item for sub_list in b for item in sub_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a0a37ef-f6a4-4799-ac5f-3d64e819c5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3262"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6a3af-a6f1-4913-adf7-a937e5ca00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_BOW_tfidf=np.zeros_like(train_vector)\n",
    "for i in range(len(vocabulary)):\n",
    "    train_BOW_tfidf[:,i] = train_vector[:,i] * idfs_BOW[i]\n",
    "    \n",
    "test_BOW_tfidf=np.zeros_like(test_vector)\n",
    "for i in range(len(vocabulary)):\n",
    "    test_BOW_tfidf[:,i] = test_vector[:,i] * idfs_BOW[i]\n",
    "\n",
    "dev_BOW_tfidf=np.zeros_like(dev_vector)\n",
    "for i in range(len(vocabulary)):\n",
    "    dev_BOW_tfidf[:,i] = dev_vector[:,i] * idfs_BOW[i]\n",
    "    \n",
    "train_BOCW_tfidf=np.zeros_like(train_vec_char)\n",
    "for i in range(len(vocab_char)):\n",
    "    train_BOCW_tfidf[:,i] = train_vec_char * idfs_BOCW[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f9ec7-8f3b-45b5-b606-90517b9903a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern='', \n",
    "                   stop_words2=[], vocab=set(), char_ngrams=False):\n",
    "    x_raw=x_raw.lower()\n",
    "    x, bigrams, trigrams=[],[],[]\n",
    "    if char_ngrams==True:\n",
    "        split_text=re.split(token_pattern,x_raw)\n",
    "        char_2ngrams, char_3ngrams= [],[]\n",
    "        sentence=list((' '.join(split_text)))\n",
    "        #print(sentence)\n",
    "        #print(list(unigrams))\n",
    "        char_2ngrams=[\"\".join(j) for j in zip(*[sentence[i:] for i in range(2)])]\n",
    "        char_3ngrams=[\"\".join(j) for j in zip(*[sentence[i:] for i in range(3)])]\n",
    "        x.append(char_2ngrams)\n",
    "        x.append(char_3ngrams)\n",
    "        #for word in unigrams:\n",
    "        #    n2grams= [word[i:i+2] for i in range(len(word)-1)]\n",
    "            #if (vocab)!=set(): n2grams= [word for word in n2grams if word in vocab]\n",
    "        #    x.append(n2grams)\n",
    "         #   n3grams= [word[i:i+3] for i in range(len(word)-2)]\n",
    "            #if (vocab)!=set(): n3grams= [word for word in n3grams if word in vocab]\n",
    "          #  x.append(n3grams)\n",
    "        x = [item for sub_list in x for item in sub_list]    # unlist items and make a single list  \n",
    "        #if vocab!=set(): x = [word for word in x if word in vocab]\n",
    "    else:   \n",
    "        split_text=re.findall(token_pattern,x_raw)\n",
    "        unigrams = [word for word in split_text if not word in stop_words2]\n",
    "        x.append(unigrams)\n",
    "        bi_grams = [unigrams[i:i+2] for i in range(len(unigrams)-1)]\n",
    "        #for w in (bi_grams): bigrams.append(' '.join(w))\n",
    "        bigrams = [item for sub_list in bi_grams for item in sub_list] \n",
    "        x.append(bigrams)\n",
    "        tri_grams = [unigrams[i:i+3] for i in range(len(unigrams)-2)]\n",
    "        trigrams = [item for sub_list in tri_grams for item in sub_list] \n",
    "        #for w in (tri_grams): trigrams.append(' '.join(w))\n",
    "        x.append(trigrams)\n",
    "        x = [item for sub_list in x for item in sub_list]    # unlist items and make a single list\n",
    "        #if vocab!=set(): x = [word for word in x if word in vocab]\n",
    "    return x\n",
    "a=extract_ngrams(train_set_list[0], stop_words2=stop_words,char_ngrams=True, vocab=vocab_char, token_pattern='\\t')\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd08a70-e442-4cd0-96ba-90b995daa7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv= CountVectorizer(analyzer='char',ngram_range=(2,3), min_df=10)\n",
    "X=cv.fit_transform(train_set_list)\n",
    "X.toarray()\n",
    "print(X.shape)\n",
    "Y=cv.transform(test_set_list)\n",
    "Y.shape\n",
    "lr.fit(X,train_labels)\n",
    "y_pp=lr.predict(Y)\n",
    "accuracy_score(y_pp,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f1610c85-2f5c-4950-b685-9a3a3b123ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "a=1\n",
    "b=10\n",
    "c=100\n",
    "if a>0 and b<19 or c>190:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "858434e3-990d-4a72-a2f1-afd3f21108f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ad': 111, 'aa': 11, 'af': 2, 'a': 1, 'ab': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aa': 11, 'ad': 111}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x={'a':1,'aa':11,'ab':0,'ad':111,'af':2}\n",
    "sab=dict(sorted(x.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sab)\n",
    "p=list(sab)[2:]\n",
    "for i in p:\n",
    "    del x[i]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46879f8-d85a-41d4-b71c-e33c06157e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "no\n",
      "no\n",
      "no\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "s=1\n",
    "e=5\n",
    "for n in range(s,e+1):\n",
    "    if n==1: print('yes')\n",
    "    else: print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f8d2cad-7c99-4bee-ad7f-3e7195b5745f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "t=9\n",
    "print(t*t) if t<7 else print(t-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b4dbd52-82d3-4ab9-97fd-e329a7ca0329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jagat is a good boy']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sen=\"Jagat is a good boy\"\n",
    "a=(re.split('\\t',sen))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "139295e3-5ebe-4f1c-bb7b-937100582d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=[1,2,3,4]\n",
    "sum((i*i for i in w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7854944f-7533-4c12-a33c-6acbff1dda81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696741854636592"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(max_iter=100000,penalty='l2')\n",
    "lr.fit(train_BOW_count,train_labels)\n",
    "y_pred=lr.predict(test_BOW_count)\n",
    "accuracy_score(test_labels,y_pred)\n",
    "#train_BOW_count[1,:].shape\n",
    "#p = np.random.permutation(len(train_BOW_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4b1d37e-1254-4419-b8ff-636f92f05f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., 12., -2.,  1.],\n",
       "       [ 6., 13., -1.,  1.],\n",
       "       [ 4., 14.,  0.,  2.],\n",
       "       [ 5., 15., -1.,  1.],\n",
       "       [ 3., 16.,  2.,  0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "a=[2,6,4,5,3]\n",
    "b=(np.linspace(12,16, num=5))\n",
    "c=[-2,-1,0,-1,2]\n",
    "d=[1,1,2,1,0]\n",
    "e=pd.DataFrame(\n",
    "    {'lst1Title': a,\n",
    "     'lst2Title': b,\n",
    "     'lst3Title': c,\n",
    "     'dfg':d\n",
    "    })\n",
    "e=e.to_numpy()\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b090443e-2abe-456b-b995-57ddc836e924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.5 ],\n",
       "       [1.  , 0.25, 0.25, 0.5 ],\n",
       "       [0.5 , 0.5 , 0.5 , 1.  ],\n",
       "       [0.75, 0.75, 0.25, 0.5 ],\n",
       "       [0.25, 1.  , 1.  , 0.  ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f=(e-np.mean(e))/np.std(e)\n",
    "for i in range(e.shape[1]):\n",
    "    p=sorted(e[:,i])\n",
    "    mini=p[0]\n",
    "    maxi=p[-1]\n",
    "    e[:,i]=(e[:,i]-mini)/(maxi-mini)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f87f6b38-d3ee-48c0-8e80-acc696934bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 4, 5, 3]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce8254b-ea3d-47d0-b9c2-6314c556f358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 'k']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4]\n",
    "b=[2,4,6]\n",
    "d={2:'p',4:'k',5:'l'}\n",
    "c=[k for k in a if k in b]\n",
    "p=[d[w] for w in c]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264160b-19ab-4d07-b689-8a9cb3c4faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(X,vocab):\n",
    "    vec=np.zeros(shape=(len(X),len(vocab)))\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(vocab)):\n",
    "            vec[i,j]=X[i].count(vocab[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7412a674-2f29-4986-8c44-58b7471c045d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pqr=[[1],[2],[3],[4]]\n",
    "np.mean(pqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cb67ba7-f9e4-4b78-8451-8d082cd1e60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Jagat', 1: 'Kiran', 2: 'Subhisha', 3: 'Sub'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[\"Jagat\",\"Kiran\"]\n",
    "b=[\"Subhisha\",\"Sub\"]\n",
    "dict(enumerate(a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de7b70-2907-499e-9450-107a18387a3b",
   "metadata": {},
   "source": [
    "But the classifier would not work well in the domains beyond that because each domain has other specific sentiment terms. For exampls, in laptop reviews, the terms like cheap price, big screen, long battery, useless touchpad, bulky weight, loud speaker, bright screen etc can't be picked up by the above classifier. \n",
    "\n",
    "For restaurant reviews, the words like airy atmosphere, tasty food, affordable, accessible location, delicious food, wait time, small quantity etc convey sentiment but our movie review classifier might not work well on the these terms. \n",
    "\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |   |   |   |\n",
    "| BOW-tfidf  |   |   |   |\n",
    "| BOCN-count  |   |   |   |\n",
    "| BOCN-tfidf  |   |   |   |\n",
    "| BOW+BOCN  |   |   |   |\n",
    "\n",
    "Please discuss why your best performing model is better than the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ed9ec-ae8e-472b-90e7-74fa5ff67379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
