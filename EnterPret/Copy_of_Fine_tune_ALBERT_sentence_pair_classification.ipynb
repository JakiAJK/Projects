{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4b37902db494497944ab5e465db52be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_261c1125e05046ccbdf66039541b8788",
              "IPY_MODEL_e1780322852f4153960c5525263de333",
              "IPY_MODEL_15b471e18fae4012b542e871bf07dc0c"
            ],
            "layout": "IPY_MODEL_b9a2cc0a8c4445788884c91635e5a04e"
          }
        },
        "261c1125e05046ccbdf66039541b8788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_244a4801a9d641d0a4589cca4ad46e96",
            "placeholder": "​",
            "style": "IPY_MODEL_addc061917be4304b62faa15da95d3e9",
            "value": "Downloading: "
          }
        },
        "e1780322852f4153960c5525263de333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41b9ce3218c341d79b34a50de66bfc80",
            "max": 1585,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9199cc31dad4f5199cb979f915b4b89",
            "value": 1585
          }
        },
        "15b471e18fae4012b542e871bf07dc0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9059c6d4c66344c7a000e95022529fa2",
            "placeholder": "​",
            "style": "IPY_MODEL_9d8de075c2d14869a2b678f1540d2924",
            "value": " 4.39k/? [00:00&lt;00:00, 169kB/s]"
          }
        },
        "b9a2cc0a8c4445788884c91635e5a04e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "244a4801a9d641d0a4589cca4ad46e96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "addc061917be4304b62faa15da95d3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41b9ce3218c341d79b34a50de66bfc80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9199cc31dad4f5199cb979f915b4b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9059c6d4c66344c7a000e95022529fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d8de075c2d14869a2b678f1540d2924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuLR0BMJhIub"
      },
      "source": [
        "# Fine-tune ALBERT for sentence-pair classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZIHhCYNhN-H"
      },
      "source": [
        "## Introduction \n",
        "\n",
        "You will learn in this notebook how to fine-tune ALBERT and other BERT-based models for the **sentence-pair classification** task. This PyTorch implementation leverages the Hugging face *transformers* and *datasets* libraries to download pre-trained models, enable quick research experiments, access datasets and evaluation metrics.\n",
        "\n",
        "This task is part of the semantic textual similarity problem. You have two pair of sentences and you want to model the textual interaction between them.\n",
        "\n",
        "The dataset used in this notebook is Microsoft Research Paraphrase Corpus (MRPC) which is part of the GLUE benchmark : you have two sentences and you want to predict if one sentence is the paraphrase of the other one. The evaluation metrics are F1 and accuracy.\n",
        "\n",
        "You should be able to reach on the validation set **91.19** as F1 score (the score reported in the ALBERT paper is 90.9) and **87.5** as accuracy. The fine-tuning takes 35 seconds per epoch and the inference takes 2 seconds.\n",
        "\n",
        "The main features of this tutorial are : \n",
        "- End-to-end ML implementation (training, validation, prediction, evaluation)\n",
        "- Easy adaptability to your own datasets\n",
        "- Facilitation of quick experiments with other BERT-based models (BERT, ALBERT, ...)\n",
        "- Quick training with limited computational resources (mixed-precision, gradient accumulation, ...)\n",
        "- Multi-GPU execution\n",
        "- Threshold choice for the classification decision (not necessarily 0.5)\n",
        "- Freeze BERT layers and only update the classification layer weights or update all the weights\n",
        "- Reproducible results with seed settings\n",
        "\n",
        "#### Sections\n",
        "\n",
        "1. [Installation of libraries and imports](#section01)\n",
        "\n",
        "2. [Loading the dataset](#section02)\n",
        "\n",
        "3. [Classes and functions](#section03)\n",
        "\n",
        "4. [Parameters](#section04)\n",
        "\n",
        "5. [Training and validation](#section05)\n",
        "\n",
        "6. [Prediction](#section06)\n",
        "\n",
        "7. [Evaluation](#section07)\n",
        "\n",
        "8. [Experiments' ideas](#section08)\n",
        "\n",
        "9. [Limitations](#section09)\n",
        "\n",
        "10. [Future works](#section10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIv7rF4V6lyE"
      },
      "source": [
        "## Installation of libraries and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42-lJ1u9IHT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688de35b-e42f-4a51-ea5a-d3aa8a42115d"
      },
      "source": [
        "!pip install datasets==1.0.1\n",
        "!pip install transformers==3.1.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets==1.0.1 in /usr/local/lib/python3.8/dist-packages (1.0.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.1) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.1) (3.9.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.1) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.1) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.1) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.1) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.1) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.1) (0.3.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.0.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.0.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.0.1) (1.26.14)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.0.1) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.1) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.8/dist-packages (from transformers==3.1.0) (0.1.97)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==3.1.0) (2.25.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers==3.1.0) (0.0.53)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==3.1.0) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==3.1.0) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==3.1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==3.1.0) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==3.1.0) (3.9.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.8/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.1.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.1.0) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.1.0) (2022.12.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.1.0) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.1.0) (8.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE_TpNaSZQ5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ecd2456-444d-49a9-fd83-ab6d2efb2422"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch version 1.13.1+cu116 available.\n",
            "TensorFlow version 2.11.0 available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF8IWdEowPP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f4cca9-49d2-4525-beb4-cd5c27417914"
      },
      "source": [
        "# Check that we are using 100% of GPU memory footprint support libraries/code\n",
        "# from https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip -q install gputil\n",
        "!pip -q install psutil\n",
        "!pip -q install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen RAM Free: 12.3 GB  | Proc size: 814.5 MB\n",
            "GPU RAM Free: 15101MB | Used: 0MB | Util   0% | Total 15360MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoGB7Arrwb4A"
      },
      "source": [
        "\n",
        "In case GPU utilisation (Util) is not at 0%, you can uncomment and run the following line to kill all processes to get the full GPU afterwards. Make sure to comment out the line again to not constantly crash the notebook on purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNdVkubWwdiD"
      },
      "source": [
        "# !kill -9 -1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpKx43Iq6znw"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0hfGEe2LB9s"
      },
      "source": [
        "# Load the MRPC dataset (train, validation and test)\n",
        "# dataset = load_dataset('glue', 'mrpc')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import random\n",
        "from time import localtime, strftime\n",
        "from scipy.stats import spearmanr,pearsonr\n",
        "import zipfile\n",
        "import gc\n",
        "import time\n",
        "# fixing random seed for reproducibility\n",
        "random.seed(210116270)\n",
        "np.random.seed(210116270)\n",
        "import warnings\n",
        "#suppress warnings like RunTime error\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "train_set= pd.read_csv(\"my_lists.csv\")\n",
        "test_set = pd.read_excel(\"evaluation.xlsx\")\n",
        "trr_set=pd.read_excel(\"train.xlsx\")\n",
        "\n",
        "train_data=train_set.iloc[:,:2]\n",
        "train_label=train_set.iloc[:,2]\n",
        "train_c1=train_data.iloc[:,0]\n",
        "train_c2=train_data.iloc[:,1]\n",
        "eval_data=test_set.iloc[:,:2]\n",
        "eval_label=test_set.iloc[:,2]\n",
        "eval_c1=eval_data.iloc[:,0]\n",
        "eval_c2=eval_data.iloc[:,1]\n",
        "\n",
        "n_train_c1=list(train_c1)\n",
        "n_train_c2=list(train_c2)\n",
        "n_train_label=list(train_label)\n",
        "\n",
        "\n",
        "train_data=train_set.iloc[:,:2]\n",
        "n_train_label=list(train_set.iloc[:,2])\n",
        "n_train_c1=list(train_data.iloc[:,0])\n",
        "n_train_c2=list(train_data.iloc[:,1])\n",
        "eval_data=test_set.iloc[:,:2]\n",
        "eval_label=list(test_set.iloc[:,2])\n",
        "eval_c1=list(eval_data.iloc[:,0])\n",
        "eval_c2=list(eval_data.iloc[:,1])\n",
        "\n",
        "train_label=list(trr_set.iloc[:,2])\n",
        "train_c1=list(trr_set.iloc[:,0])\n",
        "train_c2=list(trr_set.iloc[:,1])\n",
        "\n",
        "\n",
        "def pick_rand(i):\n",
        "    return random.choice(list(range(1, i-20)) + list(range(i+20, len(train_label))))\n",
        "\n",
        "for i in range(len(train_c1)):\n",
        "    n_train_c1.append(train_c1[i])\n",
        "    n_train_c2.append(train_c2[pick_rand(i)])\n",
        "    n_train_label.append(0)\n",
        "\n",
        "\n",
        "c = list(zip(n_train_c1, n_train_c2, n_train_label))\n",
        "\n",
        "random.shuffle(c)\n",
        "\n",
        "n_train_c1, n_train_c2, n_train_label = zip(*c)\n",
        "n_train_c1=list(n_train_c1)\n",
        "n_train_c2=list(n_train_c2)\n",
        "n_train_label=list(n_train_label)"
      ],
      "metadata": {
        "id": "CYJ34N_UFTOf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpobOzx-Lht7"
      },
      "source": [
        "#split = dataset['train'].train_test_split(test_size=0.1, seed=1)  # split the original training data for validation\n",
        "#train = split['train']  # 90 % of the original training data\n",
        "#val = split['test']   # 10 % of the original training data\n",
        "#test = dataset['validation']  # the original validation data is used as test data because the test labels are not available with the datasets library\n",
        "\n",
        "# Transform data into pandas dataframes\n",
        "\n",
        "tr_data=[]\n",
        "for i in range(len(n_train_label)):\n",
        "    tr_data.append([i+1, n_train_label[i], n_train_c1[i], n_train_c2[i]])\n",
        "te_data=[]\n",
        "for i in range(len(eval_label)):\n",
        "    te_data.append([i+1, eval_label[i] ,eval_c1[i], eval_c2[i]])\n",
        "\n",
        "df_train = pd.DataFrame(tr_data)\n",
        "df_train.columns=[\"idx\",\"label\",\"sentence1\",\"sentence2\"]\n",
        "df_val = pd.DataFrame(te_data)\n",
        "df_val.columns=[\"idx\",\"label\",\"sentence1\",\"sentence2\"]\n",
        "df_test = df_val"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsjCBsJFF4Kn"
      },
      "source": [
        "If you want to use your own dataset, you can upload it on the left of the notebook. \n",
        "\n",
        "For now, only csv files are handled, you need to upload three files : training data, validation data and test data (with or without labels)\n",
        "\n",
        "Here is a script to load data from csv files with the headers below : \n",
        "- sentence1\n",
        "- sentence2 \n",
        "- label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCA-bKfuEbjH"
      },
      "source": [
        "# Load your dataset from csv files\n",
        "\n",
        "# Some useful UNIX commands : \n",
        "# !pwd -> print working directory\n",
        "# !ls -> list directory contents of files and directories\n",
        "# %cd -> change the directory/folder of the terminal's shell\n",
        "\n",
        "\n",
        "# path_to_train_data = '/content/...'\n",
        "# path_to_val_data = '/content/...'\n",
        "# path_to_test_data = '/content/...'\n",
        "\n",
        "# delimiter = \";\" \n",
        "\n",
        "# df_train = pd.read_csv(path_to_train_data, delimiter=delimiter)\n",
        "# df_val = pd.read_csv(path_to_val_data, delimiter=delimiter)\n",
        "# df_test = pd.read_csv(path_to_test_data, delimiter=delimiter)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNs2FWNJSLSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "317f0ee0-0eef-449c-90fd-eebf9def5cbe"
      },
      "source": [
        "print(df_train.shape)\n",
        "print(df_val.shape)\n",
        "print(df_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8744, 4)\n",
            "(9000, 4)\n",
            "(9000, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irj7itV0UCF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a4ef2ade-9b86-4c37-8e30-515c0e8a87e8"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   idx  label                                          sentence1  \\\n",
              "0    1      1  it is nice to have a star wars and marvel cate...   \n",
              "1    2      0  very annoying not being able to get a full scr...   \n",
              "2    3      0        but i can not link my cell phone to the tv.   \n",
              "3    4      1                  missing information is unfriendly   \n",
              "4    5      0  it does not allow creating groups section amon...   \n",
              "\n",
              "                                       sentence2  \n",
              "0  good for watching star wars and marvel movies  \n",
              "1                    facing sound related issues  \n",
              "2           app is bad and download time is slow  \n",
              "3                     missing information is bad  \n",
              "4                    unable to enter profile pin  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b7ac12c-80de-4214-a98c-d0f5efd22b16\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>label</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>it is nice to have a star wars and marvel cate...</td>\n",
              "      <td>good for watching star wars and marvel movies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>very annoying not being able to get a full scr...</td>\n",
              "      <td>facing sound related issues</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>but i can not link my cell phone to the tv.</td>\n",
              "      <td>app is bad and download time is slow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>missing information is unfriendly</td>\n",
              "      <td>missing information is bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>it does not allow creating groups section amon...</td>\n",
              "      <td>unable to enter profile pin</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b7ac12c-80de-4214-a98c-d0f5efd22b16')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b7ac12c-80de-4214-a98c-d0f5efd22b16 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b7ac12c-80de-4214-a98c-d0f5efd22b16');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWJfh6DV7CB5"
      },
      "source": [
        "## Classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc1GQh7yEm4C"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n",
        "\n",
        "        self.data = data  # pandas dataframe\n",
        "        #Initialize the tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n",
        "\n",
        "        self.maxlen = maxlen\n",
        "        self.with_labels = with_labels \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Selecting sentence1 and sentence2 at the specified index in the data frame\n",
        "        sent1 = str(self.data.loc[index, 'sentence1'])\n",
        "        sent2 = str(self.data.loc[index, 'sentence2'])\n",
        "\n",
        "        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n",
        "        encoded_pair = self.tokenizer(sent1, sent2, \n",
        "                                      padding='max_length',  # Pad to max_length\n",
        "                                      truncation=True,  # Truncate to max_length\n",
        "                                      max_length=self.maxlen,  \n",
        "                                      return_tensors='pt')  # Return torch.Tensor objects\n",
        "        \n",
        "        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor of token ids\n",
        "        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # binary tensor with \"0\" for padded values and \"1\" for the other values\n",
        "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  # binary tensor with \"0\" for the 1st sentence tokens & \"1\" for the 2nd sentence tokens\n",
        "\n",
        "        if self.with_labels:  # True if the dataset has labels\n",
        "            label = self.data.loc[index, 'label']\n",
        "            return token_ids, attn_masks, token_type_ids, label  \n",
        "        else:\n",
        "            return token_ids, attn_masks, token_type_ids"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm0lAXvTZChm"
      },
      "source": [
        "class SentencePairClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
        "        super(SentencePairClassifier, self).__init__()\n",
        "        #  Instantiating BERT-based model object\n",
        "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
        "\n",
        "        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n",
        "        if bert_model == \"albert-base-v2\":  # 12M parameters\n",
        "            hidden_size = 768\n",
        "        elif bert_model == \"albert-large-v2\":  # 18M parameters\n",
        "            hidden_size = 1024\n",
        "        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n",
        "            hidden_size = 2048\n",
        "        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n",
        "            hidden_size = 4096\n",
        "        elif bert_model == \"bert-base-uncased\": # 110M parameters\n",
        "            hidden_size = 768\n",
        "\n",
        "        # Freeze bert layers and only train the classification layer weights\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # Classification layer\n",
        "        self.cls_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    @autocast()  # run in mixed precision\n",
        "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -input_ids : Tensor  containing token ids\n",
        "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
        "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
        "        '''\n",
        "\n",
        "        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n",
        "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
        "\n",
        "        # Feeding to the classifier layer the last layer hidden-state of the [CLS] token further processed by a\n",
        "        # Linear Layer and a Tanh activation. The Linear layer weights were trained from the sentence order prediction (ALBERT) or next sentence prediction (BERT)\n",
        "        # objective during pre-training.\n",
        "        logits = self.cls_layer(self.dropout(pooler_output))\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SrSNNYTjwe8"
      },
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "\n",
        "def evaluate_loss(net, device, criterion, dataloader):\n",
        "    net.eval()\n",
        "\n",
        "    mean_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "            logits = net(seq, attn_masks, token_type_ids)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            count += 1\n",
        "\n",
        "    return mean_loss / count"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl-rhuWsrg01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93937ae-ef68-4547-bc8d-49d1776d9c97"
      },
      "source": [
        "print(\"Creation of the models' folder...\")\n",
        "!mkdir models"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creation of the models' folder...\n",
            "mkdir: cannot create directory ‘models’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsL9VvmX0NLC"
      },
      "source": [
        "Link for mixed precision training, gradient scaling and gradient accumulation  : https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples\n",
        "\n",
        "If you would like to learn more about Training Neural Nets on Larger Batches, I suggest reading this post of Thomas Wolf :\n",
        "https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-o6KyaFkU5u"
      },
      "source": [
        "def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
        "\n",
        "    best_loss = np.Inf\n",
        "    best_ep = 1\n",
        "    nb_iterations = len(train_loader)\n",
        "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
        "    iters = []\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n",
        "\n",
        "            # Converting to cuda tensors\n",
        "            seq, attn_masks, token_type_ids, labels = \\\n",
        "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
        "    \n",
        "            # Enables autocasting for the forward pass (model + loss)\n",
        "            with autocast():\n",
        "                # Obtaining the logits from the model\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "\n",
        "                # Computing loss\n",
        "                loss = criterion(logits.squeeze(-1), labels.float())\n",
        "                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (it + 1) % iters_to_accumulate == 0:\n",
        "                # Optimization step\n",
        "                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n",
        "                # otherwise, opti.step() is skipped.\n",
        "                scaler.step(opti)\n",
        "                # Updates the scale for next iteration.\n",
        "                scaler.update()\n",
        "                # Adjust the learning rate based on the number of iterations.\n",
        "                lr_scheduler.step()\n",
        "                # Clear gradients\n",
        "                opti.zero_grad()\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (it + 1) % print_every == 0:  # Print training loss information\n",
        "                print()\n",
        "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
        "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
        "\n",
        "                running_loss = 0.0\n",
        "\n",
        "\n",
        "        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
        "        print()\n",
        "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
        "            print()\n",
        "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
        "            best_loss = val_loss\n",
        "            best_ep = ep + 1\n",
        "\n",
        "    # Saving the model\n",
        "    path_to_model='models/{}_lr_{}_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), best_ep)\n",
        "    torch.save(net_copy.state_dict(), path_to_model)\n",
        "    print(\"The model has been saved in {}\".format(path_to_model))\n",
        "\n",
        "    del loss\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFy9kQ2-SvQ2"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6bzDp4FreS6"
      },
      "source": [
        "bert_model = \"albert-xlarge-v2\" #\"albert-base-v2\"  # 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2', 'bert-base-uncased', ...\n",
        "freeze_bert = False  # if True, freeze the encoder weights and only update the classification layer weights\n",
        "maxlen = 256 #128  # maximum length of the tokenized input sentence pair : if greater than \"maxlen\", the input is truncated and else if smaller, the input is padded\n",
        "bs = 2  # batch size\n",
        "iters_to_accumulate = 2  # the gradient accumulation adds gradients over an effective batch of size : bs * iters_to_accumulate. If set to \"1\", you get the usual batch size\n",
        "lr = 2e-6  # learning rate\n",
        "epochs = 2  # number of training epochs"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_abThXlSr6n"
      },
      "source": [
        "## Training and validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwXCoj9_h1hY"
      },
      "source": [
        "Link for the AdamW optimizer and the learning rate scheduler :\n",
        "https://huggingface.co/transformers/main_classes/optimizer_schedules.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZWGPomoryxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c922e1a-3f1c-4b0d-dc97-acbaf67b21fa"
      },
      "source": [
        "#  Set all seeds to make reproducible results\n",
        "set_seed(1)\n",
        "\n",
        "# Creating instances of training and validation set\n",
        "print(\"Reading training data...\")\n",
        "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
        "print(\"Reading validation data...\")\n",
        "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
        "# Creating instances of training and validation dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size=bs, num_workers=5)\n",
        "val_loader = DataLoader(val_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
        "\n",
        "if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    net = nn.DataParallel(net)\n",
        "\n",
        "net.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
        "num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
        "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
        "t_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading training data...\n",
            "Reading validation data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 20%|█▉        | 874/4372 [04:56<19:33,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 874/4372 of epoch 1 complete. Loss : 0.20136723482397384 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 1748/4372 [09:49<14:43,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 1748/4372 of epoch 1 complete. Loss : 0.07857978358775275 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 2622/4372 [14:41<09:50,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 2622/4372 of epoch 1 complete. Loss : 0.05204247748047684 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 3496/4372 [19:34<04:58,  2.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 3496/4372 of epoch 1 complete. Loss : 0.038009704179098526 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 4370/4372 [24:27<00:00,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 4370/4372 of epoch 1 complete. Loss : 0.030114619755713605 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4372/4372 [24:28<00:00,  2.98it/s]\n",
            "100%|██████████| 4500/4500 [08:59<00:00,  8.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 complete! Validation Loss : 0.8979904384613037\n",
            "Best validation loss improved from inf to 0.8979904384613037\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 874/4372 [04:53<19:32,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 874/4372 of epoch 2 complete. Loss : 0.027017032144057353 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 1748/4372 [09:45<14:45,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 1748/4372 of epoch 2 complete. Loss : 0.022066098213242687 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 2622/4372 [14:38<09:50,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 2622/4372 of epoch 2 complete. Loss : 0.01898357102147131 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 3496/4372 [19:30<04:55,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 3496/4372 of epoch 2 complete. Loss : 0.014636022678775675 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 4370/4372 [24:23<00:00,  2.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 4370/4372 of epoch 2 complete. Loss : 0.010280494235729845 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4372/4372 [24:24<00:00,  2.99it/s]\n",
            "100%|██████████| 4500/4500 [08:59<00:00,  8.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 complete! Validation Loss : 1.0062130923800998\n",
            "The model has been saved in models/albert-xlarge-v2_lr_2e-06_val_loss_0.89799_ep_1.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw2sOrIvCEZz"
      },
      "source": [
        "You can download the model saved in the folder \"models\" by browsing the files on the left of the colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f4zThMtvqC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba01fd5-5b17-4669-f322-1b031a224fb8"
      },
      "source": [
        "# If you encounter a CUDA out of memory error: \n",
        "# - uncomment the kill command, run the \"kill\" command (and comment it)\n",
        "# - reduce the batch size\n",
        "# - then run all cells from the begining \n",
        "\n",
        "# If you get an ugly print of tqdm (all iterations are showed), follow the above first and last steps\n",
        "\n",
        "printm()\n",
        "# !kill -9 -1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gen RAM Free: 9.3 GB  | Proc size: 3.6 GB\n",
            "GPU RAM Free: 15101MB | Used: 0MB | Util   0% | Total 15360MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDBtVu7JSbUK"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAfbt0FjkCfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b04dd8e-16f8-4fe6-aaba-07f75c5caa79"
      },
      "source": [
        "print(\"Creation of the results' folder...\")\n",
        "!mkdir results"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creation of the results' folder...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3r8_npVf30D"
      },
      "source": [
        "def get_probs_from_logits(logits):\n",
        "    \"\"\"\n",
        "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "def test_prediction(net, device, dataloader, with_labels=True, result_file=\"results/output.txt\"):\n",
        "    \"\"\"\n",
        "    Predict the probabilities on a dataset with or without labels and print the result in a file\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    w = open(result_file, 'w')\n",
        "    probs_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if with_labels:\n",
        "            for seq, attn_masks, token_type_ids, _ in tqdm(dataloader):\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "        else:\n",
        "            for seq, attn_masks, token_type_ids in tqdm(dataloader):\n",
        "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
        "                logits = net(seq, attn_masks, token_type_ids)\n",
        "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                probs_all += probs.tolist()\n",
        "\n",
        "    w.writelines(str(prob)+'\\n' for prob in probs_all)\n",
        "    w.close()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-VJObbxwL6r"
      },
      "source": [
        "I'm sharing below an ALBERT pre-trained model (45 Mo) so you can reproduce my results on the MRPC validation set (**91.19** as F1 score and **87.5** as accuracy). It's just in case but if all the code run as expected, you should get after the model training the correct model in the *models* folder\n",
        "\n",
        "You can download it and upload it (~ 3 minutes) in the *models* folder by browsing the files on the left of the colab notebook :\n",
        "\n",
        "https://drive.google.com/file/d/1AcRLGvALAH3BVSiDVjY_b8CggJgVfksp/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWoWiw6MlPm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8f92a0-4ed2-45ae-a898-81244eb624a7"
      },
      "source": [
        "path_to_model = '/content/models/albert-xlarge-v2_lr_2e-06_val_loss_0.89799_ep_1.pt'  \n",
        "\n",
        "# path_to_model = '/content/models/...'  # You can add here your trained model\n",
        "\n",
        "path_to_output_file = 'results/output.txt'\n",
        "\n",
        "print(\"Reading test data...\")\n",
        "test_set = CustomDataset(df_test, maxlen, bert_model)\n",
        "test_loader = DataLoader(test_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "model = SentencePairClassifier(bert_model)\n",
        "if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "print()\n",
        "print(\"Loading the weights of the model...\")\n",
        "model.load_state_dict(torch.load(path_to_model))\n",
        "model.to(device)\n",
        "\n",
        "print(\"Predicting on test data...\")\n",
        "test_prediction(net=model, device=device, dataloader=test_loader, with_labels=True,  # set the with_labels parameter to False if your want to get predictions on a dataset without labels\n",
        "                result_file=path_to_output_file)\n",
        "print()\n",
        "print(\"Predictions are available in : {}\".format(path_to_output_file))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading test data...\n",
            "\n",
            "Loading the weights of the model...\n",
            "Predicting on test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4500/4500 [09:00<00:00,  8.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions are available in : results/output.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCVAtClcC1qT"
      },
      "source": [
        "You can download the predictions saved in the folder \"results\" by browsing the files on the left of the colab notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywxq1c8DSiV3"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JYwEPtrlBFX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "a4b37902db494497944ab5e465db52be",
            "261c1125e05046ccbdf66039541b8788",
            "e1780322852f4153960c5525263de333",
            "15b471e18fae4012b542e871bf07dc0c",
            "b9a2cc0a8c4445788884c91635e5a04e",
            "244a4801a9d641d0a4589cca4ad46e96",
            "addc061917be4304b62faa15da95d3e9",
            "41b9ce3218c341d79b34a50de66bfc80",
            "b9199cc31dad4f5199cb979f915b4b89",
            "9059c6d4c66344c7a000e95022529fa2",
            "9d8de075c2d14869a2b678f1540d2924"
          ]
        },
        "outputId": "3c10e6a8-f85e-4aee-ef6c-0db2637a6c80"
      },
      "source": [
        "path_to_output_file = 'results/output.txt'  # path to the file with prediction probabilities\n",
        "\n",
        "labels_test = df_test['label']  # true labels\n",
        "\n",
        "probs_test = pd.read_csv(path_to_output_file, header=None)[0]  # prediction probabilities\n",
        "threshold = 0.5   # you can adjust this threshold for your own dataset\n",
        "preds_test=(probs_test>=threshold).astype('uint8') # predicted labels using the above fixed threshold\n",
        "\n",
        "metric = load_metric(\"glue\", \"mrpc\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/tmpubvv1wcw\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4b37902db494497944ab5e465db52be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py in cache at /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py\n",
            "creating metadata file for /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py\n",
            "Checking /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py for additional imports.\n",
            "Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6\n",
            "Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.py\n",
            "Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/dataset_infos.json\n",
            "Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "preds_te=preds_test\n",
        "\n",
        "print('Accuracy:', accuracy_score(labels_test,preds_te))\n",
        "print('Precision:', precision_score(labels_test,preds_te))\n",
        "print('Recall:', recall_score(labels_test,preds_te))\n",
        "print('F1-Score:', f1_score(labels_test,preds_te))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(labels_test,preds_te)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2WnrY5ULQi1",
        "outputId": "3596cb92-70af-4efc-be89-6fe0d99e1273"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6808888888888889\n",
            "Precision: 0.5118587975730833\n",
            "Recall: 0.9276907697434189\n",
            "F1-Score: 0.6597156398104266\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3344, 2655],\n",
              "       [ 217, 2784]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LGJRzdN7UOT"
      },
      "source": [
        "Link for the threshold choice problem : https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3ReQL2CUj3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872fd54a-85c8-4a63-f1ec-bdbcf7e03f3d"
      },
      "source": [
        "# Compute the accuracy and F1 scores\n",
        "metric._compute(predictions=preds_test, references=labels_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.6808888888888889, 'f1': 0.6597156398104266}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ExhSydG0eeq"
      },
      "source": [
        "## Experiments' ideas\n",
        "\n",
        "- Try other pre-trained models: https://huggingface.co/models\n",
        "- Try other optimizers and learning rate schedulers\n",
        "- Tune the hyperparameters : batch size, gradient accumulation parameter (iters_to_accumulate), number of epochs, learning rate\n",
        "- Change the *maxlen* parameter (max : 512). If you increase it, the training will take longer\n",
        "- Observe the influence of freezing the encoder weights and only updating the classifier weights\n",
        "- Use other metrics (Precision, Recall, ROC AUC, Precision-recall AUC, etc.) depending on the task and the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtkcmIY9t6AF"
      },
      "source": [
        "## Limitations\n",
        "\n",
        "- As said in the BERT github repository of Google Research (https://github.com/google-research/bert), \"Small sets like MRPC have a high variance in the Dev set accuracy, even when starting from the same re-training checkpoint.\"\n",
        "So I suggest taking that into account if you want to compare models on this dataset.\n",
        "- Distinct random seeds for models trained on GLUE datasets including MRPC can have a significant impact on results : for more details, you can read the paper *Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping* by Dodge et al. (https://arxiv.org/abs/2002.06305)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZp5xdVwvWVC"
      },
      "source": [
        "## Future works\n",
        "\n",
        "- Adapt the code so other BERT-based models like RoBERTa and DistillBERT can also be fine-tuned for this task\n",
        "- Experiment feeding to the classification layer the last layer hidden states' average of all input tokens or other operations with multiple encoder layers instead of the pooler output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QzFESYepPaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}